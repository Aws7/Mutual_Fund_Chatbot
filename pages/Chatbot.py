# from langchain_community.chat_models import ChatCohere
from langchain_cohere import ChatCohere
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_cohere import CohereEmbeddings
import json
import PyPDF2
import streamlit as st
import os
from dotenv import load_dotenv

st.set_page_config("ChatSDK Fund", "ğŸ’¬")

load_dotenv()

# API Keys
# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
COHERE_API_KEY = os.getenv('COHERE_API_KEY')

# Prompt message
base_prompt = """
Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ ÙˆØ£Ø³Ù…Ùƒ ÙˆØ«Ù‚. Ø£Ù†Øª ØªØªØ¨Ø¹ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø§Ù†Øª ØªØªØ­Ø¯Ø« Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· ÙˆÙ„Ø§ ØªØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø§ÙŠ Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¥ Ø§Ø°Ø§ ÙƒØ§Ù† Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆÙ„Ø§Ù…Ø§Ù†Ø¹ Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø®Ù„ÙŠØ· Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØºÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„Ù„ØºØ§Øª Ø§Ùˆ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª. Ø£Ù†Øª ØªØ¯Ø±Ø³ Ø¬Ù…ÙŠØ¹ Ù…Ø§ ÙŠØ¹Ø·Ù‰ Ù„Ùƒ ÙˆØªØ¹Ø·ÙŠ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ù‡ ÙˆÙ…ÙØµÙ„Ø© Ø­Ø³Ø¨ Ø§Ù„Ø§Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ùˆ Ø§Ù„Ù‚Ø¶ÙŠØ©. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ù† Ø­ÙŠØ«ÙŠØ§Øª Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù†ÙØ³Ù‡ Ø­ØªÙ‰ ØªØªØ¶Ø­ Ù„Ùƒ Ø§Ù„ØµÙˆØ±Ø© ÙƒØ§Ù…Ù„Ø© ÙˆØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¥Ø¯Ù„Ø§Ø¡ Ø¨Ù†Øµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø³ØªÙ†Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© ÙˆØ§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø¯Ø§Ø¦Ù…Ø§ Ø§Ø±ÙØ§Ù‚ Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ Ø¨Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ùˆ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙƒÙ…Ø±Ø¬Ø¹ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆÙƒÙ…Ø­Ø§Ù…ÙŠ Ù…Ø­ØªØ±Ù. Ù„Ø§ØªØ¬Ø¨ Ø¹Ù„Ù‰ Ø£ÙŠ Ø³Ø¤Ø§Ù„ ØºÙŠØ± Ù…Ø±ØªØ¨Ø· Ø¨Ùƒ ÙƒÙ…Ø­Ø§Ù…ÙŠ ÙˆØ¨Ø¥Ù…ÙƒØ§Ù†Ùƒ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Support@wtheq.sa ÙˆÙŠÙƒÙˆÙ† Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„ ÙÙŠ Ø³Ø·Ø± Ø®Ø§Øµ Ø¨Ù‡. ØªØ°ÙƒØ± Ø£Ù† ØªÙ‚Ø¯Ù… Ù†ÙØ³Ùƒ Ø¯Ø§Ø¦Ù…Ø§ ÙƒÙ…Ø­Ø§Ù…ÙŠ ÙˆØ§Ø³Ù…Ùƒ ÙˆØ«Ù‚.
"""

# Using Cohere's embed-english-v3.0 embedding model
embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY, model="embed-english-v3.0")

# For OpenAI's gpt-3.5-turbo llm
# llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)

# For Cohere's command-r llm
llm = ChatCohere(temperature=0, cohere_api_key=COHERE_API_KEY, model="command-r")

# For reading PDFs and returning text string
def read_pdf(files):
    file_content = ""
    for file in files:
        # Create a PDF file reader object
        pdf_reader = PyPDF2.PdfReader(file)
        # Get the total number of pages in the PDF
        num_pages = len(pdf_reader.pages)
        # Iterate through each page and extract text
        for page_num in range(num_pages):
            # Get the page object
            page = pdf_reader.pages[page_num]
            file_content += page.extract_text()
    return file_content

# -----------------------------------------------------------#
# ------------------------ğŸ’¬ CHATBOT -----------------------#
# ----------------------------------------------------------#

def chatbot():
    st.subheader("Ask questions from the PDFs")
    st.markdown("<br>", unsafe_allow_html=True)

    # Write previous conversations
    for i in st.session_state.conversation_chatbot:
        user_msg = st.chat_message("human", avatar="ğŸ’")
        user_msg.write(i[0])
        computer_msg = st.chat_message("ai", avatar="ğŸ§ ")
        computer_msg.write(i[1])

    prompt = st.chat_input("Say something")
    
    if prompt:
        exprompt = base_prompt + "\n\n" + prompt
        if st.session_state.book_docsearch:
            exprompt += " . This is regarding the uploaded file."

        user_text = prompt
        user_msg = st.chat_message("human", avatar="ğŸ’")
        user_msg.write(user_text)

        with st.spinner("Getting Answer..."):
            if st.session_state.book_docsearch:
                # No of chunks the search should retrieve from the db
                chunks_to_retrieve = 5
                retriever = st.session_state.book_docsearch.as_retriever(search_type="similarity", search_kwargs={"k": chunks_to_retrieve})

                ## RetrievalQA Chain ##
                qa = RetrievalQA.from_llm(llm=llm, retriever=retriever, verbose=True)
                answer = qa({"query": exprompt})["result"]
            else:
                answer = llm(exprompt)

            computer_text = answer
            computer_msg = st.chat_message("ai", avatar="ğŸ§ ")
            computer_msg.write(computer_text)

            if st.session_state.book_docsearch:
                # Showing chunks with score
                doc_score = st.session_state.book_docsearch.similarity_search_with_score(prompt, k=chunks_to_retrieve)
                with st.popover("See chunks..."):
                    st.write(doc_score)

            # Adding current conversation_chatbot to the list.
            st.session_state.conversation_chatbot.append((prompt, answer))

# For initialization of session variables
def initial(flag=False):
    path = "db"
    if 'existing_indices' not in st.session_state or flag:
        st.session_state.existing_indices = [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]
    if 'selected_option' not in st.session_state or flag:
        try:
            st.session_state.selected_option = st.session_state.existing_indices[0]
        except:
            st.session_state.selected_option = None
    if 'conversation_chatbot' not in st.session_state:
        st.session_state.conversation_chatbot = []
    if 'book_docsearch' not in st.session_state:
        st.session_state.book_docsearch = None

def main():
    initial(True)
    # Streamlit UI
    st.title("ğŸ’° Mutual Fund Chatbot")

    # For showing the index selector
    file_list = []
    for index in st.session_state.existing_indices:
        with open(f"db/{index}/desc.json", "r") as openfile:
            description = json.load(openfile)
            file_list.append(",".join(description["file_names"]))

    with st.popover("Select index", help="ğŸ‘‰ Select the datastore from which data will be retrieved"):
        st.session_state.selected_option = st.radio("Select a Document...", st.session_state.existing_indices, captions=file_list, index=0)

    st.write(f"*Selected index* : **:orange[{st.session_state.selected_option}]**")

    # Load the selected index from local storage
    if st.session_state.selected_option:
        st.session_state.book_docsearch = FAISS.load_local(f"db/{st.session_state.selected_option}", embeddings, allow_dangerous_deserialization=True)

    # Call the chatbot function
    chatbot()

main()
